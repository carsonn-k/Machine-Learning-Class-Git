---
title: "PA 4.1"
format:
  html:
    embed-resources: true
---

# XML, HTML, and Web Scraping

JSON and XML are two different ways to represent hierarchical data. Which one is better? There are lots of articles online which discuss similarities and differences between JSON and XML and their advantages and disadvantages. Both formats are still in current usage, so it is good to be familiar with both. However, JSON is more common, so we'll focus on working with JSON representations of hierarchical data. 

The reading covered an example of using Beautiful Soup to parse XML. Rather than doing another example XML now, we'll skip straight to scraping HTML from a webpage. Both HTML and XML can be parsed in a similar way with Beautiful Soup.


```{python}
import pandas as pd
import requests
```

# Scraping an HTML table with Beautiful Soup

Open the URL https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population and scroll down until you see a table of the cities in the U.S. with population over 100,000 (as of Jul 1, 2022). We'll use Beautiful Soup to scrape information from this table.

Read in the HTML from the ULR using the requests library.


```{python}
URL = "https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population"
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

response = requests.get(URL, headers=HEADERS)

df = pd.read_html(response.text)

# df
```

Use Beautiful Soup to parse this string into a tree called soup


```{python}
from bs4 import BeautifulSoup
soup = BeautifulSoup(response.text, "html.parser")
```

To find an HTML tag corresponding to a specific element on a webpage, right-click on it and choose "Inspect element". Go to the cities table Wikipedia page and do this now.

You should find that the cities table on the Wikipedia page corresponds to the element


There are many table tags on the page.

```{python}
len(soup.find_all("table"))
```

We can use attributes like class= and style= to narrow down the list.


```{python}
len(soup.find_all("table",
                  attrs={
                      "class": "sortable wikitable sticky-header-multi static-row-numbers sort-under col1left col2center",
                      "style": "text-align:right"}
                  ))
```

At this point, you can manually inspect the tables on the webpage to find that the one we want is the first one (see [0] below). We'll store this as table.


```{python}
table = soup.find_all("table",
                  attrs={
                      "class": "sortable wikitable sticky-header-multi static-row-numbers sort-under col1left col2center",
                      "style": "text-align:right"}
                  )[0]
```

Now you will write code to scrape the information in table to create a Pandas data frame with one row for each city and columns for: city, state, population (2022 estimate), and 2020 land area (sq mi). Refer to the Notes/suggestions below as you write your code. A few Hints are provided further down, but try coding first before looking at the hints.

Notes/suggestions:

Use as a guide the code from the reading that produced the data frame of Statistics faculty
Inspect the page source as you write your code
You will need to write a loop to get the information for all cities, but you might want to try just scraping the info for New York first
You will need to pull the text from the tag. If .text returns text with "\n" at the end, try .get_text(strip = True) instead of .text
Don't forget to convert to a Pandas Data Frame; it should have 333 rows and 4 columns
The goal of this exercise is just to create the Data Frame. If you were going to use it --- e.g., what is the population density for all cities in CA? --- then you would need to clean the data first (to clean strings and convert to quantitative). (You can use Beautiful Soup to do some of the cleaning for you, but that goes beyond our scope.)


```{python}
tables = soup.find_all("table")
len(tables)
```


```{python}
table = tables[2]
```


```{python}
city = table.find_all("tr")
len(city)
```

```{python}
city = table.find_all("tr")[2]
# city
```

```{python}
cells = city.find_all("td")
# cells
```


```{python}
cells[0].find("a")
```


```{python}
cells[0].find("a").text
```


```{python}
cells[1].find("a").text
```


```{python}
cells[3].get_text(strip = True)
```


```{python}
cells[5].get_text(strip = True)
```


```{python}
# initialize an empty list
rows = []

# iterate over all rows in the faculty table
for city in table.find_all("tr")[2:]:

    # Get all the cells (<td>) in the row.
    cells = city.find_all("td")

    # The information we need is the text between tags.

    # Find the the name of the faculty in cell[0]
    # which for most faculty is contained in the <strong> tag
    city = cells[0].find("a").text

    # Find the office of the faculty in cell[1]
    # which for most faculty is contained in the <a> tag
    state = cells[1].find("a").text

    # Find the email of the faculty in cell[3]
    # which for most faculty is contained in the <a> tag
    pop = cells[3].get_text(strip = True) 

    area = cells[5].get_text(strip = True) 

    # Append this data.
    rows.append({
        "city": city,
        "state": state,
        "pop": pop,
        "area": area
    })

pd.DataFrame(rows)
```

Hints:

Each city is a row in the table; find all the tr tags to find all the cities
Look for the td tag to see table entries within a row
The rank column is represented by th tags, rather than td tags. So within a row, the first (that is, [0]) td tag corresponds to the city name.

# Aside: Scraping an HTML table with Pandas
The Pandas command read_html can be used to scrape information from an HTML table on a webpage.

We can call read_html on the URL.


```{python}
URL = "https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population"
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

response = requests.get(URL, headers=HEADERS)
```

However, this scrapes all the tables on the webpage, not just the one we want. As with Beautiful Soup, we can narrow the search by specifying the table attributes.


```{python}
URL = "https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population"
attrs = {'class': 'wikitable sortable', "style": "text-align:center"}
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

response = requests.get(URL, attrs, headers=HEADERS)
```

This still returns 3 tables. As we remarked above, the table that we want is the first one (see [0] below).


```{python}
df_cities2 = pd.read_html(response.text)

df_cities2[0]
```

This did not work properly as the webpage html has changed since writing this assignment, there were 3 expected tables but more were returned and table 0 isnt the one desired.

Wait, that seemed much easier than using Beautiful Soup, and it returned a data frame, and we even got for free some formatting like removing the commas from the population! Why didn't we just use read_html in the first place? It's true the read_html works well when scraping information from an HTML table. Unfortunately, you often want to scrape information from a webpage that isn't conveniently stored in an HTML table, in which case read_html won't work. (It only searches for table, th, tr, and td tags, but there are many other HTML tags.) Though Beautiful Soup is not as simple as read_html, it is more flexible and thus more widely applicable.

Scraping information that is NOT in a table with Beautiful Soup
The Cal Poly course catalog http://catalog.calpoly.edu/collegesandprograms/collegeofsciencemathematics/statistics/#courseinventory contains a list of courses offered by the Statistics department. You will scrape this website to obtain a Pandas data frame with one row for each DATA or STAT course and two columns: course name and number (e.g, DATA 301. Introduction to Data Science) and term typically offered (e.g., Term Typically Offered: F, W, SP).

Note: Pandas read_html is not help here since the courses are not stored in a table.


```{python}
pd.read_html("http://catalog.calpoly.edu/collegesandprograms/collegeofsciencemathematics/statistics/#courseinventory")
```

Notes/suggestions:

Inspect the page source as you write your code
The courses are not stored in a <table>. How are they stored?
You will need to write a loop to get the information for all courses, but you might want to try just scraping the info for DATA 100 first
What kind of tag is the course name stored in? What is the class of the tag?
What kind of tag is the quarter(s) the course is offered stored in? What is the class of the tag? Is this the only tag of this type with the class? How will you get the one you want?
You don't have to remove the number of units (e.g., 4 units) from the course name and number, but you can try it if you want
You will need to pull the text from the tag. If .text returns text with "\n" at the end, try get_text(strip = True) instead of text
Don't forget to convert to a Pandas Data Frame; it should have 74 rows and 2 columns
The goal of this exercise is just to create the Data Frame. If you were going to use it then you might need to clean the data first. (You can use Beautiful Soup to do some of the cleaning for you, but that goes beyond our scope.)


```{python}
URL = "https://catalog.calpoly.edu/collegesandprograms/collegeofsciencemathematics/statistics/#courseinventory"
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

response = requests.get(URL, headers=HEADERS)

df = pd.read_html(response.text)

# df
```


```{python}
soup = BeautifulSoup(response.text, "html.parser")
```


```{python}
len(soup.find_all("table"))
```


```{python}
len(soup.find_all(
                  attrs={
                      "class": "courses"}
                  ))
```


```{python}
table = soup.find_all(
                  attrs={
                      "class": "courses"}
                  )[0]
```


```{python}
row = table.find_all("p")
len(row)
```


```{python}
courses = soup.find_all("div", {"class": "courseblock"})
# courses
```

```{python}
rows = []

for course in courses:
    title_tag = course.find("p", class_="courseblocktitle")
    title = title_tag.get_text(strip=True)

    offerd_tags = course.find_all("p", class_="noindent")
    offered = offerd_tags[0].get_text(strip=True)

    rows.append({
        "Course Title": title,
        "Typically Offered": offered
    })

df = pd.DataFrame(rows)
df
```