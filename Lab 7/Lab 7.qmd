---
title: "Lab 7"
format:
  html:
    embed-resources: true
code-fold: false
toc: true
---


```{python}
import pandas as pd

ha = pd.read_csv("https://www.dropbox.com/s/aohbr6yb9ifmc8w/heart_attack.csv?dl=1")

df = ha
df.describe()
```

```{python}
df_clean = df.dropna()
df_clean.head
```

# Part 1

## Knn
```{python}
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.neighbors import KNeighborsClassifier

X = df_clean.drop(columns=['output'])
y = df_clean['output']

ct = ColumnTransformer(
  [
    ("dummify", OneHotEncoder(sparse_output = False), ["cp", "restecg", "sex"]),
    ("standardize", StandardScaler(), ["age", "chol", "trtbps", "thalach"])
  ],
  remainder = "passthrough"
)

knn_pipeline = Pipeline(
  [("preprocessing", ct),
  ("knn", KNeighborsClassifier())]
)

knn_pipeline

knn_pipeline_fitted = knn_pipeline.fit(X, y)

y_preds_knn = knn_pipeline_fitted.predict(X)
```

```{python}
#| output: false

from sklearn.model_selection import GridSearchCV

param_grid_knn = {
    'knn__n_neighbors': [3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29],
    'knn__weights': ['uniform', 'distance'],
    'knn__metric': ['euclidean', 'manhattan']
}

knn_grid_search = GridSearchCV(knn_pipeline, param_grid_knn, cv=5, scoring='roc_auc')

knn_grid_search_fitted = knn_grid_search.fit(X, y)
knn_df_cv_results_ = pd.DataFrame(knn_grid_search_fitted.cv_results_)

knn_df_cv_results_
```

```{python}
from sklearn.model_selection import cross_val_score

knn_scores = cross_val_score(knn_pipeline, X, y, cv = 5, scoring = 'roc_auc')

knn_scores
```

```{python}
import numpy as np

roc_auc_knn = np.mean(knn_scores)
print(f"Cross Validated ROC AUC {roc_auc_knn}")
```

```{python}
knn_pipeline.set_params(knn__n_neighbors=23, knn__weights='uniform', knn__metric='manhattan')
knn_pipeline_fitted = knn_pipeline.fit(X, y)
y_preds_knn = knn_pipeline_fitted.predict(X)
```

```{python}
from sklearn.metrics import confusion_matrix

knn_cm = confusion_matrix(y, y_preds_knn)
print(knn_cm)
```

Here we can see that the model correctly predicted 104 true posatives, 110 true negatives, and had 23 false posatives with 36 false negatives.

## Logistic

```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

lr_pipeline = Pipeline(
  [("preprocessing", ct),
  ("logistic_regression", LogisticRegression())]
)

lr_pipeline

lr_pipeline_fitted = lr_pipeline.fit(X, y)

y_preds_log = lr_pipeline_fitted.predict(X)
```

```{python}
from sklearn.model_selection import GridSearchCV

param_grid_log = {
    'logistic_regression__C': [0.01, 0.1, 1, 10, 100]
}

log_grid_search = GridSearchCV(lr_pipeline, param_grid_log, cv=5, scoring='roc_auc')

log_grid_search_fitted = log_grid_search.fit(X, y)
log_df_cv_results_ = pd.DataFrame(log_grid_search_fitted.cv_results_)

log_df_cv_results_
```

```{python}
from sklearn.model_selection import cross_val_score

log_scores = cross_val_score(lr_pipeline, X, y, cv = 5, scoring = 'roc_auc')

log_scores
```

```{python}
import numpy as np

roc_auc_log = np.mean(log_scores)
print(f"Cross Validated ROC AUC {roc_auc_log}")
```



```{python}
lr_pipeline.set_params(logistic_regression__C=1)
lr_pipeline_fitted = lr_pipeline.fit(X, y)
y_preds_log = lr_pipeline_fitted.predict(X)
```

```{python}
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y, y_preds_log)
print(cm)
```

```{python}
model_lr = lr_pipeline.named_steps["logistic_regression"]
feature_names = lr_pipeline[:-1].get_feature_names_out()
coefficients = model_lr.coef_
coefficients = coefficients.squeeze()
lr_coef_df = pd.DataFrame({
    'predictor': feature_names,
    'coefficient': coefficients
})
lr_coef_df = lr_coef_df.sort_values(by='coefficient', ascending=False)

lr_coef_df
```

Here we can see the impacts of each coefficient. The most impactfull are sex, thalach, and CP. Sex causes close to a 1 point swing depending on male or female with male being the one that adds liklihood. A 1 point increase in maximum heart rate achieved during exercise (thalach) causes a .76 increase in predicted output. 

## Lasso 

```{python}
from sklearn.linear_model import Lasso

lasso_clf = LogisticRegression(
    penalty = "l1",          
    solver = "saga",
    max_iter = 10000
)


lasso_pipeline = Pipeline([
    ("preprocess", ct),
    ("lasso_clf", lasso_clf)
])


lasso_pipeline

lasso_pipeline_fitted = lasso_pipeline.fit(X, y)

y_preds_lasso = lasso_pipeline_fitted.predict(X)
```

```{python}
from sklearn.model_selection import GridSearchCV

param_grid = {
    "lasso_clf__C": [0.001, 0.01, 0.1, 1, 10]
}

lasso_grid_search = GridSearchCV(lasso_pipeline, param_grid, cv=5, scoring='roc_auc')

lasso_grid_search_fitted = lasso_grid_search.fit(X, y)
lasso_df_cv_results_ = pd.DataFrame(lasso_grid_search_fitted.cv_results_)

lasso_df_cv_results_
```

```{python}
from sklearn.model_selection import cross_val_score

lasso_scores = cross_val_score(lasso_pipeline, X, y, cv = 5, scoring = 'roc_auc')

lasso_scores
```

```{python}
import numpy as np

roc_auc_lasso = np.mean(lasso_scores)
print(f"Cross Validated ROC AUC {roc_auc_lasso}")

```

```{python}
lasso_pipeline.set_params(lasso_clf__C=1)
lasso_pipeline_fitted = lasso_pipeline.fit(X, y)
y_preds_lasso = lasso_pipeline_fitted.predict(X)
```

```{python}
from sklearn.metrics import confusion_matrix

lasso_cm = confusion_matrix(y, y_preds_lasso)
print(lasso_cm)
```

```{python}
model_lasso = lasso_pipeline.named_steps["lasso_clf"]
feature_names = lasso_pipeline[:-1].get_feature_names_out()
coefficients = model_lasso.coef_
coefficients = coefficients.squeeze()
lasso_coef_df = pd.DataFrame({
    'predictor': feature_names,
    'coefficient': coefficients
})
lasso_coef_df = lasso_coef_df.sort_values(by='coefficient', ascending=False)

lasso_coef_df
```

## Ridge

```{python}
from sklearn.linear_model import RidgeClassifier

ridge_pipeline = Pipeline([
    ("preprocess", ct),
    ("ridge_clf", RidgeClassifier())
])


ridge_pipeline

ridge_pipeline_fitted = ridge_pipeline.fit(X, y)

y_preds_ridge = ridge_pipeline_fitted.predict(X)

```

```{python}
from sklearn.model_selection import GridSearchCV

param_grid = {
    "ridge_clf__alpha": [0.001, 0.01, 0.1, 1, 10]
}

ridge_grid_search = GridSearchCV(ridge_pipeline, param_grid, cv=5, scoring='roc_auc')

ridge_grid_search_fitted = ridge_grid_search.fit(X, y)
ridge_df_cv_results_ = pd.DataFrame(ridge_grid_search_fitted.cv_results_)

ridge_df_cv_results_
```

```{python}
from sklearn.model_selection import cross_val_score

ridge_scores = cross_val_score(ridge_pipeline, X, y, cv=5, scoring='roc_auc')

ridge_scores
```

```{python}
import numpy as np

roc_auc_ridge = np.mean(ridge_scores)
print(f"Cross Validated ROC AUC {roc_auc_ridge}")
```

```{python}
ridge_pipeline.set_params(ridge_clf__alpha=10)
ridge_pipeline_fitted = ridge_pipeline.fit(X, y)
y_preds_ridge = ridge_pipeline_fitted.predict(X)
```

```{python}
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y, y_preds_ridge)
print(cm)
```

```{python}
model_ridge = ridge_pipeline.named_steps["ridge_clf"]
feature_names = ridge_pipeline[:-1].get_feature_names_out()
coefficients = model_ridge.coef_
coefficients = coefficients.squeeze()
ridge_coef_df = pd.DataFrame({
    'predictor': feature_names,
    'coefficient': coefficients
})
ridge_coef_df = ridge_coef_df.sort_values(by='coefficient', ascending=False)

ridge_coef_df
```

## Elastic

```{python}
from sklearn.linear_model import LogisticRegression

elastic_clf = LogisticRegression(
    penalty='elasticnet',    
    solver='saga',           
    l1_ratio=0.5,          
    max_iter=10000
)


elastic_pipeline = Pipeline([
    ("preprocess", ct),
    ("elastic_clf", elastic_clf)
])


elastic_pipeline

elastic_pipeline_fitted = elastic_pipeline.fit(X, y)

y_preds_elastic = elastic_pipeline_fitted.predict(X)
```

```{python}
#| output: false

from sklearn.model_selection import GridSearchCV

param_grid = {
    "elastic_clf__C": [0.001, 0.01, 0.1, 1, 10],
    "elastic_clf__l1_ratio": [0.2, 0.5, 0.8]
}

elastic_grid_search = GridSearchCV(elastic_pipeline, param_grid, cv=5, scoring='roc_auc')

elastic_grid_search_fitted = elastic_grid_search.fit(X, y)
elastic_df_cv_results_ = pd.DataFrame(elastic_grid_search_fitted.cv_results_)

elastic_df_cv_results_
```

```{python}
from sklearn.model_selection import cross_val_score

elastic_scores = cross_val_score(elastic_pipeline, X, y, cv=5, scoring='roc_auc')

elastic_scores
```

```{python}
import numpy as np

roc_auc_elastic = np.mean(elastic_scores)
print(f"Cross Validated ROC AUC {roc_auc_elastic}")
```

```{python}
elastic_pipeline.set_params(elastic_clf__C=1, elastic_clf__l1_ratio=0.8)
elastic_pipeline_fitted = elastic_pipeline.fit(X, y)
y_preds_elastic = elastic_pipeline_fitted.predict(X)
```

```{python}
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y, y_preds_elastic)
print(cm)
```

```{python}
model_elastic = elastic_pipeline.named_steps["elastic_clf"]
feature_names = elastic_pipeline[:-1].get_feature_names_out()
coefficients = model_elastic.coef_
coefficients = coefficients.squeeze()
elastic_coef_df = pd.DataFrame({
    'predictor': feature_names,
    'coefficient': coefficients
})
elastic_coef_df = elastic_coef_df.sort_values(by='coefficient', ascending=False)

elastic_coef_df
```

## Tree

```{python}
from sklearn.tree import DecisionTreeClassifier

tree_pipeline = Pipeline(
  [("preprocessing", ct),
  ("tree", DecisionTreeClassifier())]
)

tree_pipeline

tree_pipeline_fitted = tree_pipeline.fit(X, y)

y_preds_tree = tree_pipeline_fitted.predict(X)
```

```{python}
#| output: false

from sklearn.model_selection import GridSearchCV

param_grid_tree = {
    "tree__max_depth": [1, 3, 7, 9],
    'tree__min_samples_leaf': [1, 2, 4, 6, 8],
    'tree__criterion': ['gini', 'entropy', 'log_loss']
    }

tree_grid_search = GridSearchCV(tree_pipeline, param_grid_tree, cv=5, scoring='roc_auc')

tree_grid_search_fitted = tree_grid_search.fit(X, y)
tree_df_cv_results_ = pd.DataFrame(tree_grid_search_fitted.cv_results_)

tree_df_cv_results_
```

```{python}
from sklearn.model_selection import cross_val_score

tree_scores = cross_val_score(tree_pipeline, X, y, cv = 5, scoring = 'roc_auc')

tree_scores
```

```{python}
import numpy as np

roc_auc_tree = np.mean(tree_scores)
print(f"Cross Validated ROC AUC {roc_auc_tree}")
```

```{python}
tree_pipeline.set_params(tree__max_depth=3, tree__min_samples_leaf=8, tree__criterion='gini')
tree_pipeline_fitted = tree_pipeline.fit(X, y)
y_preds_tree = tree_pipeline_fitted.predict(X)
```

```{python}
from sklearn.metrics import confusion_matrix

tree_cm = confusion_matrix(y, y_preds_tree)
print(tree_cm)
```

Here we can see that the model correctly predicted 101 true posatives, 117 true negatives, and had 26 false posatives with 29 false negatives.



```{python}
print(f"Cross Validated ROC AUC - Lasso: {roc_auc_lasso}")
print(f"Cross Validated ROC AUC - Knn: {roc_auc_knn}")
print(f"Cross Validated ROC AUC - Tree: {roc_auc_tree}")
print(f"Confusion Matrix - Lasso: \n{lasso_cm}")
print(f"Confusion Matrix - Knn: \n{knn_cm}")
print(f"Confusion Matrix - Tree: \n{tree_cm}")
```

## Interpretation
The predictors most effective in predicting heart attack risk were sex, thalach, cp and trtbps. These had the most extreme (highest absolute value) coeffecients for predicting heart attack risk. 

I went with a few tactics to find the best model. I started with the logistic model and compared it to a lasso, ridge, and elastic model. The best model of those was the lasso one. 

For the knn model I used grid search with the parameters knn__weights, knn__n_neighbors, and knn__metric. The best model had the following parameters: knn__n_neighbors=23, knn__weights='uniform', and knn__metric='manhattan'.

For the decision tree I took a similar approach for the parameter tuning with grid search. I used the tree__max_depth, tree__min_samples_leaf, and tree__criterion parameters. I ended up fitting the model with the best of these parameters. 

Overall, my lasso model had a cross validated roc auc of 0.868393, my knn had a cross validated roc auc of 0.77238, and my decision tree had a cross validated roc auc of 0.685632.

Based on these numbers my Lasso model with the parameters listed above is my best model. 

## ROC curve
```{python}
from sklearn.metrics import roc_curve, roc_auc_score
from plotnine import *
import pandas as pd


lasso_probs = lasso_pipeline.predict_proba(X)[:, 1]
lasso_fpr, lasso_tpr, _ = roc_curve(y, lasso_probs)
lasso_auc = roc_auc_score(y, lasso_probs)

knn_probs = knn_pipeline.predict_proba(X)[:, 1]
knn_fpr, knn_tpr, _ = roc_curve(y, knn_probs)
knn_auc = roc_auc_score(y, knn_probs)

# Tree
tree_probs = tree_pipeline.predict_proba(X)[:, 1]
tree_fpr, tree_tpr, _ = roc_curve(y, tree_probs)
tree_auc = roc_auc_score(y, tree_probs)

roc_df = pd.concat([
    pd.DataFrame({"fpr": lasso_fpr, "tpr": lasso_tpr, "Model": f"Lasso (AUC={lasso_auc:.3f})"}),
    pd.DataFrame({"fpr": knn_fpr, "tpr": knn_tpr, "Model": f"KNN (AUC={knn_auc:.3f})"}),
    pd.DataFrame({"fpr": tree_fpr, "tpr": tree_tpr, "Model": f"Tree (AUC={tree_auc:.3f})"})
])

(ggplot(roc_df, aes(x="fpr", y="tpr", color="Model"))
  + geom_line()
  + geom_abline(intercept=0, slope=1, linetype="dashed")
  + labs(
        x="False Positive Rate",
        y="True Positive Rate",
        color="Model"
    )
  + theme_minimal()
)
```

# Part 2
```{python}
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import precision_score, recall_score, confusion_matrix

y_pred_lasso_cv = cross_val_predict(lasso_pipeline, X, y, cv=5)
y_pred_knn_cv = cross_val_predict(knn_pipeline, X, y, cv=5)
y_pred_tree_cv = cross_val_predict(tree_pipeline, X, y, cv=5)

lasso_precision = precision_score(y, y_pred_lasso_cv)
knn_precision = precision_score(y, y_pred_knn_cv)
tree_precision = precision_score(y, y_pred_tree_cv)

lasso_recall = recall_score(y, y_pred_lasso_cv)
knn_recall = recall_score(y, y_pred_knn_cv)
tree_recall = recall_score(y, y_pred_tree_cv)

def tnr(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return tn / (tn + fp)

lasso_tnr = tnr(y, y_pred_lasso_cv)
knn_tnr = tnr(y, y_pred_knn_cv)
tree_tnr = tnr(y, y_pred_tree_cv)

print(f"Lasso  — Precision: {lasso_precision:.3f}, Recall: {lasso_recall:.3f}, TNR: {lasso_tnr:.3f}")
print(f"KNN    — Precision: {knn_precision:.3f}, Recall: {knn_recall:.3f}, TNR: {knn_tnr:.3f}")
print(f"Tree   — Precision: {tree_precision:.3f}, Recall: {tree_recall:.3f}, TNR: {tree_tnr:.3f}")
```

```{python}
from sklearn.metrics import accuracy_score

lasso_acc = accuracy_score(y, y_preds_lasso)
knn_acc   = accuracy_score(y, y_preds_knn)
tree_acc  = accuracy_score(y, y_preds_tree)

print(f"Lasso Accuracy: {lasso_acc:.3f}")
print(f"KNN Accuracy:   {knn_acc:.3f}")
print(f"Tree Accuracy:  {tree_acc:.3f}")

```

# Part 3
## Q1
We need to minimize false negatives meaning we would maximize recall. Our model with the highest recall is lasso so I would reccommend this one. On future observatios I would expect a recall score for lasso to be around 0.815 with some small fluctuation up or down. 

## Q2
We need to maximize the predicted posative rate, meaning we would maximize precision. Our model with the highest precision is knn so I would reccommend this one. On future observations I would expect a precision score of around 0.803 with some fluctuation up or down. 

## Q3
For this I would look at my roc auc score and the coefficients of the predictors in the model. The model with the best roc auc score was my lasso model so I would reccomend this one. When looking at its coeficients we can see that sex, thalach, and CP, had the highest impact. On future observatinos I would expect the roc auc score to be around 0.86839 with some fluctuation up or down. 

## Q4
For this I would look at Accuracy. This is becasue it will tell us how good our model prediction compares to the actual values. The model with the best accuracy is the decision tree so I would reccommend this one. On future observations I would expect the accuracy score around 0.8 with some fluctuation up or down. 

# Part 4

```{python}
ha_validation = pd.read_csv("https://www.dropbox.com/s/jkwqdiyx6o6oad0/heart_attack_validation.csv?dl=1")

X_val = ha_validation.drop(columns=['output'])
y_val = ha_validation['output']
```


```{python}
y_pred_lasso  = lasso_pipeline.predict(X_val)
y_proba_lasso = lasso_pipeline.predict_proba(X_val)[:, 1]

y_pred_knn  = knn_pipeline.predict(X_val)
y_proba_knn = knn_pipeline.predict_proba(X_val)[:, 1]

y_pred_tree  = tree_pipeline.predict(X_val)
y_proba_tree = tree_pipeline.predict_proba(X_val)[:, 1]
```


```{python}
from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score

def evaluate_model(name, y_true, y_pred, y_proba):
    cm = confusion_matrix(y_true, y_pred)
    roc_auc = roc_auc_score(y_true, y_proba)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)

    print(f"--- {name} ---")
    print("Confusion Matrix:")
    print(cm)
    print(f"ROC AUC: {roc_auc:.3f}")
    print(f"Precision: {precision:.3f}")
    print(f"Recall: {recall:.3f}")
    print()

evaluate_model("Lasso", y_val, y_pred_lasso, y_proba_lasso)
evaluate_model("KNN", y_val, y_pred_knn, y_proba_knn)
evaluate_model("Decision Tree", y_val, y_pred_tree, y_proba_tree)
```

My ROC AUC scores from training to validation set were basically the same. The recall scores were noticeably lower for the validation set than the training set. The precision was noticeably higher for the validation set than the training set. I would say that even with the differences in scores they were approximately correct. One explanation for the difference could be the small sample size in the validation set. 

# Part 5

```{python}
from sklearn.metrics import cohen_kappa_score

kappa_lasso = cohen_kappa_score(y, y_preds_lasso)
kappa_knn   = cohen_kappa_score(y, y_preds_knn)
kappa_tree  = cohen_kappa_score(y, y_preds_tree)

print("Cohen Kappa (Lasso):", kappa_lasso)
print("Cohen Kappa (KNN):  ", kappa_knn)
print("Cohen Kappa (Tree): ", kappa_tree)
```

From my research, it looks like Cohen Kappa is a measure used to measure the level of agreement between two sets of data. It looks like its interpreted as the level of chance that the two data sets are randomly correlated. So a score of 1 means no chance that its randomly correlated, and a score of 0 means all the correlation is up to chance. One situation it would be useful to use Cohens Kapps is when comparing model predicted value to true human diagnosed value. This is because we are comparing the agreement between the two and we are factoring in the chance that there is random correlation. This could relate to Q4 in part 3. If we were to use Cohens Kappa as the metric we could actuallt still choose the same model (tree) so it wouldnt impact what model we chose in this case. 